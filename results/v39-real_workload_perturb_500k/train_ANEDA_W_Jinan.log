PYTHON_COMMAND: python train.py --model_class aneda --model_name ANEDA --embedding_filename node2vec_dim64_epochs1_unweighted.embeddings --data_dir W_Jinan --query_dir real_workload_perturb_500k --log_dir ../results/v39-real_workload_perturb_500k --eval_runs 0 --seed 1234 --device cuda --time_limit 5 --learning_rate 0.03 --epochs 12000 --validate
Resolving data_dir: `W_Jinan` --> `../data/W_Jinan`
Resolving query_dir: `real_workload_perturb_500k` --> `../data/W_Jinan/real_workload_perturb_500k`
Resolving num_workers: `None` --> `16`
Resolving embedding_filename: `node2vec_dim64_epochs1_unweighted.embeddings` --> `../data/W_Jinan/node2vec_dim64_epochs1_unweighted.embeddings`
Arguments:
  - model_class         : aneda
  - model_name          : ANEDA
  - data_dir            : ../data/W_Jinan
  - query_dir           : ../data/W_Jinan/real_workload_perturb_500k
  - batch_size_train    : 1024
  - batch_size_test     : 1048576
  - learning_rate       : 0.03
  - optimizer_type      : adam
  - loss_function       : mse
  - epochs              : 12000
  - time_limit          : 5.0
  - validate            : True
  - eval_runs           : 0
  - display_step        : 5
  - seed                : 1234
  - device              : cuda
  - num_workers         : 16
  - log_dir             : ../results/v39-real_workload_perturb_500k
  - debug               : False
  - embedding_dim       : 64
  - p_norm              : 1
  - landmark_selection  : random
  - gnn_layer           : gat
  - disable_edge_weight : False
  - aggregation_method  : concat
  - embedding_filename  : ../data/W_Jinan/node2vec_dim64_epochs1_unweighted.embeddings
  - select_landmarks_from_train: False
  - distance_measure    : inv_dotproduct
Seed set to 1234
[92mReading edges: ../data/W_Jinan/W_Jinan.edges[0m
[93mWarning: The node ids are left-shifted by 1 (i.e., node ids start from `0 to n-1` instead of `1 to n`) in the loaded graph.[0m
Detected delimiter: `,`
[92mReading nodes: ../data/W_Jinan/W_Jinan.nodes[0m
[93mWarning: The node ids are left-shifted by 1 (i.e., node ids start from `0 to n-1` instead of `1 to n`) in the loaded graph.[0m
Detected delimiter: `,`
Loaded Graph:
  - Data name: W_Jinan
  - No. of nodes: 8908
  - No. of edges: 14070
  - Nodes data: [(0, {'feature': [505678.5149882232, 4056089.911714096]}), (1, {'feature': [505770.08172435686, 4056030.848836959]}), (5877, {'feature': [505645.733199374, 4055649.9546133857]}), (8058, {'feature': [505755.737373046, 4056095.176733334]}), (8071, {'feature': [505589.5880390714, 4056077.65136935]})]...
  - Edges data: [(0, 1, {'weight': 122.0}), (0, 5877, {'weight': 459.0}), (0, 8058, {'weight': 77.0}), (0, 8071, {'weight': 93.0}), (1, 1113, {'weight': 52.0})]...
Edgelist.shape:  (14070, 3)
Node Attributes.shape:  (8908, 2)
[92mReading dataset: ../data/W_Jinan/real_workload_perturb_500k/W_Jinan_train.queries[0m
[93mWarning: The node ids are left-shifted by 1 (i.e., node ids start from `0 to n-1` instead of `1 to n`) in the loaded graph.[0m
Detected delimiter: `,`
  - Total number of queries: 400000
[92mReading dataset: ../data/W_Jinan/real_workload_perturb_500k/W_Jinan_test.queries[0m
[93mWarning: The node ids are left-shifted by 1 (i.e., node ids start from `0 to n-1` instead of `1 to n`) in the loaded graph.[0m
Detected delimiter: `,`
  - Total number of queries: 100000
Distance matrix of size (400000,) created successfully
Replicating queries to reach target size
  - Original size: 100000
  - New size:      1000000
Distance matrix of size (1000000,) created successfully
Train dataset...
  - No. of samples: 400000
  - Min/Max distance: 1.00/29148.00
  - Mean/Std distance: 4821.99/2811.19
Test dataset...
  - No. of samples: 1000000
  - Min/Max distance: 1.00/26982.00
  - Mean/Std distance: 4826.76/2812.58
Creating train dataloader...
  - Batch size: 1024
  - No. of batches: 391
  - Number of workers: 16
  - Pin memory: True
Creating val dataloader...
  - Batch size: 1024
  - No. of batches: 977
  - Number of workers: 16
  - Pin memory: True
Creating test dataloader...
  - Batch size: 1048576
  - No. of batches: 1
  - Number of workers: 16
  - Pin memory: True
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/histogram_distances_W_Jinan_real_workload_perturb_500k.png[0m
Detected delimiter: `,`
[92mReading embeddings: ../data/W_Jinan/node2vec_dim64_epochs1_unweighted.embeddings[0m
[93mWarning: The node ids are left-shifted by 1 (i.e., node ids start from `0 to n-1` instead of `1 to n`) in the loaded graph.[0m
  - Embeddings shape: (8908, 64)
Initializing ANEDA with distance measure: inv_dotproduct, p=1, max_distance=29148.0
Initializing node embeddings from provided attributes. Shape: torch.Size([8908, 64])
Model Summary:
OptimizedModule(
  (_orig_mod): ANEDA(
    (embedding): Embedding(8908, 64)
  )
)
Model parameters size: 570112
Loss function: MSELoss()
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.03
    maximize: False
    weight_decay: 0
)
Using Device: cuda
Starting training...
Epoch:  1/12000, Batch:   78 (1024 samples), Train Loss:    844245.25
Epoch:  1/12000, Batch:  156 (1024 samples), Train Loss:    533951.88
Epoch:  1/12000, Batch:  234 (1024 samples), Train Loss:    421128.69
Epoch:  1/12000, Batch:  312 (1024 samples), Train Loss:    233191.31
Epoch:  1/12000, Batch:  390 (1024 samples), Train Loss:    126612.75
Epoch:  1/12000, Time elapsed/remaining/total: 0.07/790.68/790.75 min, Train Loss:    895292.30, Val MRE: 12.17%
Epoch:  2/12000, Batch:   78 (1024 samples), Train Loss:    107032.75
Epoch:  2/12000, Batch:  156 (1024 samples), Train Loss:     87356.41
Epoch:  2/12000, Batch:  234 (1024 samples), Train Loss:     89754.77
Epoch:  2/12000, Batch:  312 (1024 samples), Train Loss:     94159.50
Epoch:  2/12000, Batch:  390 (1024 samples), Train Loss:    105442.60
Epoch:  2/12000, Time elapsed/remaining/total: 0.13/751.44/751.56 min, Train Loss:    115642.59, Val MRE: 11.28%
Epoch:  3/12000, Batch:   78 (1024 samples), Train Loss:     76510.10
Epoch:  3/12000, Batch:  156 (1024 samples), Train Loss:     81145.56
Epoch:  3/12000, Batch:  234 (1024 samples), Train Loss:    108473.05
Epoch:  3/12000, Batch:  312 (1024 samples), Train Loss:     94331.95
Epoch:  3/12000, Batch:  390 (1024 samples), Train Loss:     74520.70
Epoch:  3/12000, Time elapsed/remaining/total: 0.18/730.89/731.07 min, Train Loss:     84455.28, Val MRE: 9.85%
Epoch:  4/12000, Batch:   78 (1024 samples), Train Loss:     85373.56
Epoch:  4/12000, Batch:  156 (1024 samples), Train Loss:     71204.11
Epoch:  4/12000, Batch:  234 (1024 samples), Train Loss:     70147.59
Epoch:  4/12000, Batch:  312 (1024 samples), Train Loss:     88252.95
Epoch:  4/12000, Batch:  390 (1024 samples), Train Loss:     77805.13
Epoch:  4/12000, Time elapsed/remaining/total: 0.24/722.95/723.19 min, Train Loss:     79936.46, Val MRE: 9.54%
Epoch:  5/12000, Batch:   78 (1024 samples), Train Loss:     95834.97
Epoch:  5/12000, Batch:  156 (1024 samples), Train Loss:     65186.75
Epoch:  5/12000, Batch:  234 (1024 samples), Train Loss:     79184.16
Epoch:  5/12000, Batch:  312 (1024 samples), Train Loss:     68496.45
Epoch:  5/12000, Batch:  390 (1024 samples), Train Loss:     79020.98
Epoch:  5/12000, Time elapsed/remaining/total: 0.30/717.21/717.51 min, Train Loss:     76419.53, Val MRE: 8.98%
Epoch:  6/12000, Batch:   78 (1024 samples), Train Loss:     64572.91
Epoch:  6/12000, Batch:  156 (1024 samples), Train Loss:     64106.01
Epoch:  6/12000, Batch:  234 (1024 samples), Train Loss:     62103.28
Epoch:  6/12000, Batch:  312 (1024 samples), Train Loss:     57208.77
Epoch:  6/12000, Batch:  390 (1024 samples), Train Loss:     63043.43
Epoch:  6/12000, Time elapsed/remaining/total: 0.36/715.68/716.04 min, Train Loss:     69282.40, Val MRE: 8.66%
Epoch:  7/12000, Batch:   78 (1024 samples), Train Loss:     68246.55
Epoch:  7/12000, Batch:  156 (1024 samples), Train Loss:     64153.38
Epoch:  7/12000, Batch:  234 (1024 samples), Train Loss:     63361.87
Epoch:  7/12000, Batch:  312 (1024 samples), Train Loss:     72441.48
Epoch:  7/12000, Batch:  390 (1024 samples), Train Loss:     57276.68
Epoch:  7/12000, Time elapsed/remaining/total: 0.42/714.64/715.05 min, Train Loss:     64322.28, Val MRE: 8.27%
Epoch:  8/12000, Batch:   78 (1024 samples), Train Loss:     53972.57
Epoch:  8/12000, Batch:  156 (1024 samples), Train Loss:     48379.13
Epoch:  8/12000, Batch:  234 (1024 samples), Train Loss:     53567.73
Epoch:  8/12000, Batch:  312 (1024 samples), Train Loss:     62277.28
Epoch:  8/12000, Batch:  390 (1024 samples), Train Loss:     56577.96
Epoch:  8/12000, Time elapsed/remaining/total: 0.48/714.81/715.29 min, Train Loss:     59570.68, Val MRE: 7.72%
Epoch:  9/12000, Batch:   78 (1024 samples), Train Loss:     58004.77
Epoch:  9/12000, Batch:  156 (1024 samples), Train Loss:     58095.73
Epoch:  9/12000, Batch:  234 (1024 samples), Train Loss:     49199.75
Epoch:  9/12000, Batch:  312 (1024 samples), Train Loss:     51166.82
Epoch:  9/12000, Batch:  390 (1024 samples), Train Loss:     49031.21
Epoch:  9/12000, Time elapsed/remaining/total: 0.54/714.89/715.42 min, Train Loss:     55134.60, Val MRE: 7.29%
Epoch: 10/12000, Batch:   78 (1024 samples), Train Loss:     53964.75
Epoch: 10/12000, Batch:  156 (1024 samples), Train Loss:     50415.80
Epoch: 10/12000, Batch:  234 (1024 samples), Train Loss:     56008.08
Epoch: 10/12000, Batch:  312 (1024 samples), Train Loss:     51267.70
Epoch: 10/12000, Batch:  390 (1024 samples), Train Loss:     50287.75
Epoch: 10/12000, Time elapsed/remaining/total: 0.59/712.80/713.39 min, Train Loss:     51791.37, Val MRE: 7.06%
Epoch: 11/12000, Batch:   78 (1024 samples), Train Loss:     52296.05
Epoch: 11/12000, Batch:  156 (1024 samples), Train Loss:     45996.64
Epoch: 11/12000, Batch:  234 (1024 samples), Train Loss:     43324.91
Epoch: 11/12000, Batch:  312 (1024 samples), Train Loss:     46338.34
Epoch: 11/12000, Batch:  390 (1024 samples), Train Loss:     54777.80
Epoch: 11/12000, Time elapsed/remaining/total: 0.66/714.45/715.11 min, Train Loss:     49661.38, Val MRE: 6.80%
Epoch: 12/12000, Batch:   78 (1024 samples), Train Loss:     52576.47
Epoch: 12/12000, Batch:  156 (1024 samples), Train Loss:     45227.35
Epoch: 12/12000, Batch:  234 (1024 samples), Train Loss:     45780.09
Epoch: 12/12000, Batch:  312 (1024 samples), Train Loss:     39960.36
Epoch: 12/12000, Batch:  390 (1024 samples), Train Loss:     48729.33
Epoch: 12/12000, Time elapsed/remaining/total: 0.71/714.21/714.93 min, Train Loss:     47340.17, Val MRE: 6.85%
Epoch: 13/12000, Batch:   78 (1024 samples), Train Loss:     44616.03
Epoch: 13/12000, Batch:  156 (1024 samples), Train Loss:     45231.02
Epoch: 13/12000, Batch:  234 (1024 samples), Train Loss:     44694.32
Epoch: 13/12000, Batch:  312 (1024 samples), Train Loss:     40755.26
Epoch: 13/12000, Batch:  390 (1024 samples), Train Loss:     41067.00
Epoch: 13/12000, Time elapsed/remaining/total: 0.77/713.50/714.28 min, Train Loss:     45428.62, Val MRE: 6.71%
Epoch: 14/12000, Batch:   78 (1024 samples), Train Loss:     43604.28
Epoch: 14/12000, Batch:  156 (1024 samples), Train Loss:     48606.29
Epoch: 14/12000, Batch:  234 (1024 samples), Train Loss:     43602.05
Epoch: 14/12000, Batch:  312 (1024 samples), Train Loss:     49726.44
Epoch: 14/12000, Batch:  390 (1024 samples), Train Loss:     51708.48
Epoch: 14/12000, Time elapsed/remaining/total: 0.84/722.52/723.36 min, Train Loss:     43258.64, Val MRE: 6.47%
Epoch: 15/12000, Batch:   78 (1024 samples), Train Loss:     39602.98
Epoch: 15/12000, Batch:  156 (1024 samples), Train Loss:     37837.74
Epoch: 15/12000, Batch:  234 (1024 samples), Train Loss:     39806.35
Epoch: 15/12000, Batch:  312 (1024 samples), Train Loss:     33398.89
Epoch: 15/12000, Batch:  390 (1024 samples), Train Loss:     40675.73
Epoch: 15/12000, Time elapsed/remaining/total: 0.91/729.49/730.41 min, Train Loss:     40706.01, Val MRE: 6.27%
Epoch: 16/12000, Batch:   78 (1024 samples), Train Loss:     37511.10
Epoch: 16/12000, Batch:  156 (1024 samples), Train Loss:     36849.93
Epoch: 16/12000, Batch:  234 (1024 samples), Train Loss:     37085.11
Epoch: 16/12000, Batch:  312 (1024 samples), Train Loss:     35161.54
Epoch: 16/12000, Batch:  390 (1024 samples), Train Loss:     32233.59
Epoch: 16/12000, Time elapsed/remaining/total: 0.99/738.33/739.32 min, Train Loss:     38302.86, Val MRE: 6.05%
Epoch: 17/12000, Batch:   78 (1024 samples), Train Loss:     37721.54
Epoch: 17/12000, Batch:  156 (1024 samples), Train Loss:     33296.93
Epoch: 17/12000, Batch:  234 (1024 samples), Train Loss:     33743.24
Epoch: 17/12000, Batch:  312 (1024 samples), Train Loss:     36146.33
Epoch: 17/12000, Batch:  390 (1024 samples), Train Loss:     34783.45
Epoch: 17/12000, Time elapsed/remaining/total: 1.06/747.36/748.42 min, Train Loss:     37407.80, Val MRE: 6.05%
Epoch: 18/12000, Batch:   78 (1024 samples), Train Loss:     38925.53
Epoch: 18/12000, Batch:  156 (1024 samples), Train Loss:     38676.91
Epoch: 18/12000, Batch:  234 (1024 samples), Train Loss:     34503.64
Epoch: 18/12000, Batch:  312 (1024 samples), Train Loss:     32993.25
Epoch: 18/12000, Batch:  390 (1024 samples), Train Loss:     31656.17
Epoch: 18/12000, Time elapsed/remaining/total: 1.15/762.76/763.90 min, Train Loss:     35122.47, Val MRE: 5.80%
Epoch: 19/12000, Batch:   78 (1024 samples), Train Loss:     32070.75
Epoch: 19/12000, Batch:  156 (1024 samples), Train Loss:     29654.98
Epoch: 19/12000, Batch:  234 (1024 samples), Train Loss:     35156.62
Epoch: 19/12000, Batch:  312 (1024 samples), Train Loss:     29485.58
Epoch: 19/12000, Batch:  390 (1024 samples), Train Loss:     30182.79
Epoch: 19/12000, Time elapsed/remaining/total: 1.20/759.64/760.85 min, Train Loss:     33740.54, Val MRE: 5.69%
Epoch: 20/12000, Batch:   78 (1024 samples), Train Loss:     35168.43
Epoch: 20/12000, Batch:  156 (1024 samples), Train Loss:     36158.45
Epoch: 20/12000, Batch:  234 (1024 samples), Train Loss:     29942.10
Epoch: 20/12000, Batch:  312 (1024 samples), Train Loss:     32294.69
Epoch: 20/12000, Batch:  390 (1024 samples), Train Loss:     32729.39
Epoch: 20/12000, Time elapsed/remaining/total: 1.26/756.72/757.99 min, Train Loss:     32505.92, Val MRE: 5.45%
Epoch: 21/12000, Batch:   78 (1024 samples), Train Loss:     27499.84
Epoch: 21/12000, Batch:  156 (1024 samples), Train Loss:     32183.37
Epoch: 21/12000, Batch:  234 (1024 samples), Train Loss:     31321.21
Epoch: 21/12000, Batch:  312 (1024 samples), Train Loss:     28008.13
Epoch: 21/12000, Batch:  390 (1024 samples), Train Loss:     32496.86
Epoch: 21/12000, Time elapsed/remaining/total: 1.32/753.76/755.08 min, Train Loss:     30652.39, Val MRE: 5.40%
Epoch: 22/12000, Batch:   78 (1024 samples), Train Loss:     28143.79
Epoch: 22/12000, Batch:  156 (1024 samples), Train Loss:     31924.88
Epoch: 22/12000, Batch:  234 (1024 samples), Train Loss:     29452.37
Epoch: 22/12000, Batch:  312 (1024 samples), Train Loss:     26572.02
Epoch: 22/12000, Batch:  390 (1024 samples), Train Loss:     27079.16
Epoch: 22/12000, Time elapsed/remaining/total: 1.38/752.66/754.04 min, Train Loss:     30069.72, Val MRE: 5.19%
Epoch: 23/12000, Batch:   78 (1024 samples), Train Loss:     27537.88
Epoch: 23/12000, Batch:  156 (1024 samples), Train Loss:     28977.16
Epoch: 23/12000, Batch:  234 (1024 samples), Train Loss:     31154.94
Epoch: 23/12000, Batch:  312 (1024 samples), Train Loss:     35081.86
Epoch: 23/12000, Batch:  390 (1024 samples), Train Loss:     28255.79
Epoch: 23/12000, Time elapsed/remaining/total: 1.44/750.98/752.42 min, Train Loss:     28502.94, Val MRE: 5.18%
Epoch: 24/12000, Batch:   78 (1024 samples), Train Loss:     29673.97
Epoch: 24/12000, Batch:  156 (1024 samples), Train Loss:     36919.26
Epoch: 24/12000, Batch:  234 (1024 samples), Train Loss:     31516.11
Epoch: 24/12000, Batch:  312 (1024 samples), Train Loss:     24958.46
Epoch: 24/12000, Batch:  390 (1024 samples), Train Loss:     25045.02
Epoch: 24/12000, Time elapsed/remaining/total: 1.50/749.91/751.41 min, Train Loss:     27573.95, Val MRE: 4.99%
Epoch: 25/12000, Batch:   78 (1024 samples), Train Loss:     25969.85
Epoch: 25/12000, Batch:  156 (1024 samples), Train Loss:     25282.27
Epoch: 25/12000, Batch:  234 (1024 samples), Train Loss:     31374.46
Epoch: 25/12000, Batch:  312 (1024 samples), Train Loss:     26622.71
Epoch: 25/12000, Batch:  390 (1024 samples), Train Loss:     25684.92
Epoch: 25/12000, Time elapsed/remaining/total: 1.56/749.56/751.12 min, Train Loss:     26269.74, Val MRE: 5.00%
Epoch: 26/12000, Batch:   78 (1024 samples), Train Loss:     26974.64
Epoch: 26/12000, Batch:  156 (1024 samples), Train Loss:     26089.04
Epoch: 26/12000, Batch:  234 (1024 samples), Train Loss:     24822.98
Epoch: 26/12000, Batch:  312 (1024 samples), Train Loss:     25699.73
Epoch: 26/12000, Batch:  390 (1024 samples), Train Loss:     22589.41
Epoch: 26/12000, Time elapsed/remaining/total: 1.62/747.58/749.20 min, Train Loss:     25225.98, Val MRE: 4.90%
Epoch: 27/12000, Batch:   78 (1024 samples), Train Loss:     22351.83
Epoch: 27/12000, Batch:  156 (1024 samples), Train Loss:     22141.63
Epoch: 27/12000, Batch:  234 (1024 samples), Train Loss:     23834.81
Epoch: 27/12000, Batch:  312 (1024 samples), Train Loss:     22906.51
Epoch: 27/12000, Batch:  390 (1024 samples), Train Loss:     23522.24
Epoch: 27/12000, Time elapsed/remaining/total: 1.68/747.04/748.72 min, Train Loss:     24173.42, Val MRE: 4.80%
Epoch: 28/12000, Batch:   78 (1024 samples), Train Loss:     23100.01
Epoch: 28/12000, Batch:  156 (1024 samples), Train Loss:     27933.58
Epoch: 28/12000, Batch:  234 (1024 samples), Train Loss:     23532.17
Epoch: 28/12000, Batch:  312 (1024 samples), Train Loss:     25826.16
Epoch: 28/12000, Batch:  390 (1024 samples), Train Loss:     23846.98
Epoch: 28/12000, Time elapsed/remaining/total: 1.74/745.61/747.36 min, Train Loss:     23723.05, Val MRE: 4.63%
Epoch: 29/12000, Batch:   78 (1024 samples), Train Loss:     22837.26
Epoch: 29/12000, Batch:  156 (1024 samples), Train Loss:     22151.19
Epoch: 29/12000, Batch:  234 (1024 samples), Train Loss:     21343.04
Epoch: 29/12000, Batch:  312 (1024 samples), Train Loss:     24087.77
Epoch: 29/12000, Batch:  390 (1024 samples), Train Loss:     21875.92
Epoch: 29/12000, Time elapsed/remaining/total: 1.80/744.78/746.58 min, Train Loss:     22922.45, Val MRE: 4.53%
Epoch: 30/12000, Batch:   78 (1024 samples), Train Loss:     22759.59
Epoch: 30/12000, Batch:  156 (1024 samples), Train Loss:     23059.35
Epoch: 30/12000, Batch:  234 (1024 samples), Train Loss:     21658.06
Epoch: 30/12000, Batch:  312 (1024 samples), Train Loss:     23806.66
Epoch: 30/12000, Batch:  390 (1024 samples), Train Loss:     21050.56
Epoch: 30/12000, Time elapsed/remaining/total: 1.87/744.36/746.22 min, Train Loss:     22014.77, Val MRE: 4.52%
Epoch: 31/12000, Batch:   78 (1024 samples), Train Loss:     25716.15
Epoch: 31/12000, Batch:  156 (1024 samples), Train Loss:     22610.53
Epoch: 31/12000, Batch:  234 (1024 samples), Train Loss:     20799.93
Epoch: 31/12000, Batch:  312 (1024 samples), Train Loss:     20002.08
Epoch: 31/12000, Batch:  390 (1024 samples), Train Loss:     19422.52
Epoch: 31/12000, Time elapsed/remaining/total: 1.93/743.59/745.51 min, Train Loss:     20657.48, Val MRE: 4.45%
Epoch: 32/12000, Batch:   78 (1024 samples), Train Loss:     18269.29
Epoch: 32/12000, Batch:  156 (1024 samples), Train Loss:     19580.21
Epoch: 32/12000, Batch:  234 (1024 samples), Train Loss:     21050.58
Epoch: 32/12000, Batch:  312 (1024 samples), Train Loss:     20770.75
Epoch: 32/12000, Batch:  390 (1024 samples), Train Loss:     20491.74
Epoch: 32/12000, Time elapsed/remaining/total: 1.99/743.30/745.29 min, Train Loss:     20077.75, Val MRE: 4.37%
Epoch: 33/12000, Batch:   78 (1024 samples), Train Loss:     19832.83
Epoch: 33/12000, Batch:  156 (1024 samples), Train Loss:     20778.26
Epoch: 33/12000, Batch:  234 (1024 samples), Train Loss:     20598.83
Epoch: 33/12000, Batch:  312 (1024 samples), Train Loss:     23585.27
Epoch: 33/12000, Batch:  390 (1024 samples), Train Loss:     17556.33
Epoch: 33/12000, Time elapsed/remaining/total: 2.05/742.93/744.98 min, Train Loss:     20035.77, Val MRE: 4.46%
Epoch: 34/12000, Batch:   78 (1024 samples), Train Loss:     20818.21
Epoch: 34/12000, Batch:  156 (1024 samples), Train Loss:     16051.29
Epoch: 34/12000, Batch:  234 (1024 samples), Train Loss:     21254.65
Epoch: 34/12000, Batch:  312 (1024 samples), Train Loss:     19779.65
Epoch: 34/12000, Batch:  390 (1024 samples), Train Loss:     19535.40
Epoch: 34/12000, Time elapsed/remaining/total: 2.11/742.88/744.99 min, Train Loss:     19406.39, Val MRE: 4.22%
Epoch: 35/12000, Batch:   78 (1024 samples), Train Loss:     17385.58
Epoch: 35/12000, Batch:  156 (1024 samples), Train Loss:     16912.48
Epoch: 35/12000, Batch:  234 (1024 samples), Train Loss:     17275.43
Epoch: 35/12000, Batch:  312 (1024 samples), Train Loss:     19495.01
Epoch: 35/12000, Batch:  390 (1024 samples), Train Loss:     17077.47
Epoch: 35/12000, Time elapsed/remaining/total: 2.17/742.39/744.56 min, Train Loss:     18258.70, Val MRE: 4.07%
Epoch: 36/12000, Batch:   78 (1024 samples), Train Loss:     19696.56
Epoch: 36/12000, Batch:  156 (1024 samples), Train Loss:     16795.91
Epoch: 36/12000, Batch:  234 (1024 samples), Train Loss:     19117.29
Epoch: 36/12000, Batch:  312 (1024 samples), Train Loss:     17649.33
Epoch: 36/12000, Batch:  390 (1024 samples), Train Loss:     15430.67
Epoch: 36/12000, Time elapsed/remaining/total: 2.23/742.27/744.50 min, Train Loss:     17912.48, Val MRE: 4.11%
Epoch: 37/12000, Batch:   78 (1024 samples), Train Loss:     19023.67
Epoch: 37/12000, Batch:  156 (1024 samples), Train Loss:     19656.06
Epoch: 37/12000, Batch:  234 (1024 samples), Train Loss:     17895.95
Epoch: 37/12000, Batch:  312 (1024 samples), Train Loss:     18522.97
Epoch: 37/12000, Batch:  390 (1024 samples), Train Loss:     15950.07
Epoch: 37/12000, Time elapsed/remaining/total: 2.29/741.35/743.64 min, Train Loss:     17649.72, Val MRE: 4.11%
Epoch: 38/12000, Batch:   78 (1024 samples), Train Loss:     16703.50
Epoch: 38/12000, Batch:  156 (1024 samples), Train Loss:     17391.55
Epoch: 38/12000, Batch:  234 (1024 samples), Train Loss:     17832.37
Epoch: 38/12000, Batch:  312 (1024 samples), Train Loss:     15696.48
Epoch: 38/12000, Batch:  390 (1024 samples), Train Loss:     15476.55
Epoch: 38/12000, Time elapsed/remaining/total: 2.35/740.56/742.91 min, Train Loss:     17008.87, Val MRE: 3.96%
Epoch: 39/12000, Batch:   78 (1024 samples), Train Loss:     20120.75
Epoch: 39/12000, Batch:  156 (1024 samples), Train Loss:     19106.22
Epoch: 39/12000, Batch:  234 (1024 samples), Train Loss:     16271.10
Epoch: 39/12000, Batch:  312 (1024 samples), Train Loss:     17183.76
Epoch: 39/12000, Batch:  390 (1024 samples), Train Loss:     18350.46
Epoch: 39/12000, Time elapsed/remaining/total: 2.41/740.17/742.58 min, Train Loss:     16687.37, Val MRE: 4.06%
Epoch: 40/12000, Batch:   78 (1024 samples), Train Loss:     15791.13
Epoch: 40/12000, Batch:  156 (1024 samples), Train Loss:     16837.42
Epoch: 40/12000, Batch:  234 (1024 samples), Train Loss:     15951.95
Epoch: 40/12000, Batch:  312 (1024 samples), Train Loss:     15783.87
Epoch: 40/12000, Batch:  390 (1024 samples), Train Loss:     15790.56
Epoch: 40/12000, Time elapsed/remaining/total: 2.47/739.36/741.83 min, Train Loss:     16504.20, Val MRE: 3.95%
Epoch: 41/12000, Batch:   78 (1024 samples), Train Loss:     16254.64
Epoch: 41/12000, Batch:  156 (1024 samples), Train Loss:     13090.06
Epoch: 41/12000, Batch:  234 (1024 samples), Train Loss:     16794.88
Epoch: 41/12000, Batch:  312 (1024 samples), Train Loss:     15960.92
Epoch: 41/12000, Batch:  390 (1024 samples), Train Loss:     18551.44
Epoch: 41/12000, Time elapsed/remaining/total: 2.53/738.45/740.98 min, Train Loss:     15930.93, Val MRE: 3.97%
Epoch: 42/12000, Batch:   78 (1024 samples), Train Loss:     15051.48
Epoch: 42/12000, Batch:  156 (1024 samples), Train Loss:     17607.56
Epoch: 42/12000, Batch:  234 (1024 samples), Train Loss:     14541.73
Epoch: 42/12000, Batch:  312 (1024 samples), Train Loss:     14844.14
Epoch: 42/12000, Batch:  390 (1024 samples), Train Loss:     14718.25
Epoch: 42/12000, Time elapsed/remaining/total: 2.59/737.06/739.65 min, Train Loss:     15144.87, Val MRE: 3.78%
Epoch: 43/12000, Batch:   78 (1024 samples), Train Loss:     14148.16
Epoch: 43/12000, Batch:  156 (1024 samples), Train Loss:     14817.16
Epoch: 43/12000, Batch:  234 (1024 samples), Train Loss:     15888.70
Epoch: 43/12000, Batch:  312 (1024 samples), Train Loss:     15292.79
Epoch: 43/12000, Batch:  390 (1024 samples), Train Loss:     12603.21
Epoch: 43/12000, Time elapsed/remaining/total: 2.65/736.44/739.09 min, Train Loss:     14974.68, Val MRE: 3.74%
Epoch: 44/12000, Batch:   78 (1024 samples), Train Loss:     14065.94
Epoch: 44/12000, Batch:  156 (1024 samples), Train Loss:     14057.92
Epoch: 44/12000, Batch:  234 (1024 samples), Train Loss:     13911.79
Epoch: 44/12000, Batch:  312 (1024 samples), Train Loss:     15464.38
Epoch: 44/12000, Batch:  390 (1024 samples), Train Loss:     15467.53
Epoch: 44/12000, Time elapsed/remaining/total: 2.71/735.91/738.62 min, Train Loss:     14440.01, Val MRE: 3.72%
Epoch: 45/12000, Batch:   78 (1024 samples), Train Loss:     13655.18
Epoch: 45/12000, Batch:  156 (1024 samples), Train Loss:     13998.42
Epoch: 45/12000, Batch:  234 (1024 samples), Train Loss:     14122.17
Epoch: 45/12000, Batch:  312 (1024 samples), Train Loss:     14788.80
Epoch: 45/12000, Batch:  390 (1024 samples), Train Loss:     13902.75
Epoch: 45/12000, Time elapsed/remaining/total: 2.77/735.83/738.60 min, Train Loss:     14410.52, Val MRE: 3.69%
Epoch: 46/12000, Batch:   78 (1024 samples), Train Loss:     16670.17
Epoch: 46/12000, Batch:  156 (1024 samples), Train Loss:     15013.81
Epoch: 46/12000, Batch:  234 (1024 samples), Train Loss:     14067.23
Epoch: 46/12000, Batch:  312 (1024 samples), Train Loss:     14014.47
Epoch: 46/12000, Batch:  390 (1024 samples), Train Loss:     14570.37
Epoch: 46/12000, Time elapsed/remaining/total: 2.83/735.93/738.77 min, Train Loss:     13954.68, Val MRE: 3.68%
Epoch: 47/12000, Batch:   78 (1024 samples), Train Loss:     12344.27
Epoch: 47/12000, Batch:  156 (1024 samples), Train Loss:     14650.39
Epoch: 47/12000, Batch:  234 (1024 samples), Train Loss:     13669.02
Epoch: 47/12000, Batch:  312 (1024 samples), Train Loss:     13947.44
Epoch: 47/12000, Batch:  390 (1024 samples), Train Loss:     12209.59
Epoch: 47/12000, Time elapsed/remaining/total: 2.89/735.85/738.75 min, Train Loss:     13492.00, Val MRE: 3.65%
Epoch: 48/12000, Batch:   78 (1024 samples), Train Loss:     13049.33
Epoch: 48/12000, Batch:  156 (1024 samples), Train Loss:     13193.24
Epoch: 48/12000, Batch:  234 (1024 samples), Train Loss:     12864.96
Epoch: 48/12000, Batch:  312 (1024 samples), Train Loss:     13032.94
Epoch: 48/12000, Batch:  390 (1024 samples), Train Loss:     12183.23
Epoch: 48/12000, Time elapsed/remaining/total: 2.95/735.44/738.39 min, Train Loss:     13144.83, Val MRE: 3.60%
Epoch: 49/12000, Batch:   78 (1024 samples), Train Loss:     14444.29
Epoch: 49/12000, Batch:  156 (1024 samples), Train Loss:     11395.55
Epoch: 49/12000, Batch:  234 (1024 samples), Train Loss:     13058.47
Epoch: 49/12000, Batch:  312 (1024 samples), Train Loss:     12150.04
Epoch: 49/12000, Batch:  390 (1024 samples), Train Loss:     13512.10
Epoch: 49/12000, Time elapsed/remaining/total: 3.01/734.75/737.76 min, Train Loss:     13032.23, Val MRE: 3.54%
Epoch: 50/12000, Batch:   78 (1024 samples), Train Loss:     12652.89
Epoch: 50/12000, Batch:  156 (1024 samples), Train Loss:     11804.06
Epoch: 50/12000, Batch:  234 (1024 samples), Train Loss:     12758.38
Epoch: 50/12000, Batch:  312 (1024 samples), Train Loss:     12551.58
Epoch: 50/12000, Batch:  390 (1024 samples), Train Loss:     11687.55
Epoch: 50/12000, Time elapsed/remaining/total: 3.07/734.84/737.91 min, Train Loss:     12718.49, Val MRE: 3.50%
Epoch: 51/12000, Batch:   78 (1024 samples), Train Loss:     10577.37
Epoch: 51/12000, Batch:  156 (1024 samples), Train Loss:     13171.02
Epoch: 51/12000, Batch:  234 (1024 samples), Train Loss:     11432.05
Epoch: 51/12000, Batch:  312 (1024 samples), Train Loss:     13392.90
Epoch: 51/12000, Batch:  390 (1024 samples), Train Loss:     12623.21
Epoch: 51/12000, Time elapsed/remaining/total: 3.14/734.95/738.09 min, Train Loss:     12395.31, Val MRE: 3.57%
Epoch: 52/12000, Batch:   78 (1024 samples), Train Loss:     11383.51
Epoch: 52/12000, Batch:  156 (1024 samples), Train Loss:     12275.54
Epoch: 52/12000, Batch:  234 (1024 samples), Train Loss:     10516.57
Epoch: 52/12000, Batch:  312 (1024 samples), Train Loss:     12333.12
Epoch: 52/12000, Batch:  390 (1024 samples), Train Loss:     10212.99
Epoch: 52/12000, Time elapsed/remaining/total: 3.20/734.84/738.04 min, Train Loss:     12302.80, Val MRE: 3.50%
Epoch: 53/12000, Batch:   78 (1024 samples), Train Loss:     12318.39
Epoch: 53/12000, Batch:  156 (1024 samples), Train Loss:     11511.55
Epoch: 53/12000, Batch:  234 (1024 samples), Train Loss:     12773.09
Epoch: 53/12000, Batch:  312 (1024 samples), Train Loss:     12117.18
Epoch: 53/12000, Batch:  390 (1024 samples), Train Loss:     13868.10
Epoch: 53/12000, Time elapsed/remaining/total: 3.26/734.68/737.94 min, Train Loss:     11959.68, Val MRE: 3.49%
Epoch: 54/12000, Batch:   78 (1024 samples), Train Loss:     12158.43
Epoch: 54/12000, Batch:  156 (1024 samples), Train Loss:     11373.58
Epoch: 54/12000, Batch:  234 (1024 samples), Train Loss:     11502.33
Epoch: 54/12000, Batch:  312 (1024 samples), Train Loss:     11865.30
Epoch: 54/12000, Batch:  390 (1024 samples), Train Loss:     11450.17
Epoch: 54/12000, Time elapsed/remaining/total: 3.32/734.28/737.60 min, Train Loss:     11831.88, Val MRE: 3.48%
Epoch: 55/12000, Batch:   78 (1024 samples), Train Loss:     11590.02
Epoch: 55/12000, Batch:  156 (1024 samples), Train Loss:     11826.23
Epoch: 55/12000, Batch:  234 (1024 samples), Train Loss:     11041.14
Epoch: 55/12000, Batch:  312 (1024 samples), Train Loss:     11898.82
Epoch: 55/12000, Batch:  390 (1024 samples), Train Loss:     11964.30
Epoch: 55/12000, Time elapsed/remaining/total: 3.38/734.31/737.69 min, Train Loss:     11402.15, Val MRE: 3.40%
Epoch: 56/12000, Batch:   78 (1024 samples), Train Loss:     11898.68
Epoch: 56/12000, Batch:  156 (1024 samples), Train Loss:     10252.34
Epoch: 56/12000, Batch:  234 (1024 samples), Train Loss:     10929.11
Epoch: 56/12000, Batch:  312 (1024 samples), Train Loss:     10298.06
Epoch: 56/12000, Batch:  390 (1024 samples), Train Loss:     11205.42
Epoch: 56/12000, Time elapsed/remaining/total: 3.44/734.31/737.75 min, Train Loss:     11169.32, Val MRE: 3.34%
Epoch: 57/12000, Batch:   78 (1024 samples), Train Loss:      9657.69
Epoch: 57/12000, Batch:  156 (1024 samples), Train Loss:     11917.58
Epoch: 57/12000, Batch:  234 (1024 samples), Train Loss:      8841.16
Epoch: 57/12000, Batch:  312 (1024 samples), Train Loss:     10891.28
Epoch: 57/12000, Batch:  390 (1024 samples), Train Loss:     10273.30
Epoch: 57/12000, Time elapsed/remaining/total: 3.50/733.85/737.35 min, Train Loss:     11021.14, Val MRE: 3.33%
Epoch: 58/12000, Batch:   78 (1024 samples), Train Loss:     10256.60
Epoch: 58/12000, Batch:  156 (1024 samples), Train Loss:      9939.86
Epoch: 58/12000, Batch:  234 (1024 samples), Train Loss:      9388.66
Epoch: 58/12000, Batch:  312 (1024 samples), Train Loss:     11072.99
Epoch: 58/12000, Batch:  390 (1024 samples), Train Loss:     11442.48
Epoch: 58/12000, Time elapsed/remaining/total: 3.56/733.79/737.35 min, Train Loss:     10821.82, Val MRE: 3.27%
Epoch: 59/12000, Batch:   78 (1024 samples), Train Loss:     11338.94
Epoch: 59/12000, Batch:  156 (1024 samples), Train Loss:     11206.37
Epoch: 59/12000, Batch:  234 (1024 samples), Train Loss:     10371.78
Epoch: 59/12000, Batch:  312 (1024 samples), Train Loss:     10755.60
Epoch: 59/12000, Batch:  390 (1024 samples), Train Loss:     11046.56
Epoch: 59/12000, Time elapsed/remaining/total: 3.63/733.85/737.48 min, Train Loss:     10564.47, Val MRE: 3.29%
Epoch: 60/12000, Batch:   78 (1024 samples), Train Loss:     11225.94
Epoch: 60/12000, Batch:  156 (1024 samples), Train Loss:     10561.12
Epoch: 60/12000, Batch:  234 (1024 samples), Train Loss:     10472.90
Epoch: 60/12000, Batch:  312 (1024 samples), Train Loss:     10325.09
Epoch: 60/12000, Batch:  390 (1024 samples), Train Loss:      9629.28
Epoch: 60/12000, Time elapsed/remaining/total: 3.69/733.86/737.54 min, Train Loss:     10331.53, Val MRE: 3.31%
Epoch: 61/12000, Batch:   78 (1024 samples), Train Loss:     10904.81
Epoch: 61/12000, Batch:  156 (1024 samples), Train Loss:     10915.85
Epoch: 61/12000, Batch:  234 (1024 samples), Train Loss:      9983.62
Epoch: 61/12000, Batch:  312 (1024 samples), Train Loss:      9895.83
Epoch: 61/12000, Batch:  390 (1024 samples), Train Loss:     11437.73
Epoch: 61/12000, Time elapsed/remaining/total: 3.75/733.85/737.60 min, Train Loss:     10377.87, Val MRE: 3.27%
Epoch: 62/12000, Batch:   78 (1024 samples), Train Loss:     10732.53
Epoch: 62/12000, Batch:  156 (1024 samples), Train Loss:      9753.75
Epoch: 62/12000, Batch:  234 (1024 samples), Train Loss:     10243.82
Epoch: 62/12000, Batch:  312 (1024 samples), Train Loss:     10188.46
Epoch: 62/12000, Batch:  390 (1024 samples), Train Loss:      9503.75
Epoch: 62/12000, Time elapsed/remaining/total: 3.81/733.89/737.70 min, Train Loss:     10254.82, Val MRE: 3.19%
Epoch: 63/12000, Batch:   78 (1024 samples), Train Loss:     10189.98
Epoch: 63/12000, Batch:  156 (1024 samples), Train Loss:      9441.09
Epoch: 63/12000, Batch:  234 (1024 samples), Train Loss:     11033.32
Epoch: 63/12000, Batch:  312 (1024 samples), Train Loss:      8978.22
Epoch: 63/12000, Batch:  390 (1024 samples), Train Loss:     11370.43
Epoch: 63/12000, Time elapsed/remaining/total: 3.87/733.87/737.74 min, Train Loss:      9981.52, Val MRE: 3.21%
Epoch: 64/12000, Batch:   78 (1024 samples), Train Loss:      9710.39
Epoch: 64/12000, Batch:  156 (1024 samples), Train Loss:     10603.21
Epoch: 64/12000, Batch:  234 (1024 samples), Train Loss:      9515.22
Epoch: 64/12000, Batch:  312 (1024 samples), Train Loss:      9918.23
Epoch: 64/12000, Batch:  390 (1024 samples), Train Loss:      9828.41
Epoch: 64/12000, Time elapsed/remaining/total: 3.94/733.93/737.87 min, Train Loss:      9664.36, Val MRE: 3.10%
Epoch: 65/12000, Batch:   78 (1024 samples), Train Loss:      9459.36
Epoch: 65/12000, Batch:  156 (1024 samples), Train Loss:      9619.20
Epoch: 65/12000, Batch:  234 (1024 samples), Train Loss:      9348.12
Epoch: 65/12000, Batch:  312 (1024 samples), Train Loss:      9058.41
Epoch: 65/12000, Batch:  390 (1024 samples), Train Loss:     10632.73
Epoch: 65/12000, Time elapsed/remaining/total: 4.00/733.88/737.87 min, Train Loss:      9812.85, Val MRE: 3.10%
Epoch: 66/12000, Batch:   78 (1024 samples), Train Loss:      8853.28
Epoch: 66/12000, Batch:  156 (1024 samples), Train Loss:      9589.06
Epoch: 66/12000, Batch:  234 (1024 samples), Train Loss:      8734.50
Epoch: 66/12000, Batch:  312 (1024 samples), Train Loss:      9500.87
Epoch: 66/12000, Batch:  390 (1024 samples), Train Loss:     10634.11
Epoch: 66/12000, Time elapsed/remaining/total: 4.06/733.84/737.89 min, Train Loss:      9386.95, Val MRE: 3.17%
Epoch: 67/12000, Batch:   78 (1024 samples), Train Loss:      8947.99
Epoch: 67/12000, Batch:  156 (1024 samples), Train Loss:      8375.69
Epoch: 67/12000, Batch:  234 (1024 samples), Train Loss:      8942.22
Epoch: 67/12000, Batch:  312 (1024 samples), Train Loss:      9739.01
Epoch: 67/12000, Batch:  390 (1024 samples), Train Loss:     10135.88
Epoch: 67/12000, Time elapsed/remaining/total: 4.12/733.82/737.94 min, Train Loss:      9180.93, Val MRE: 3.19%
Epoch: 68/12000, Batch:   78 (1024 samples), Train Loss:      9946.69
Epoch: 68/12000, Batch:  156 (1024 samples), Train Loss:      8826.57
Epoch: 68/12000, Batch:  234 (1024 samples), Train Loss:      8305.23
Epoch: 68/12000, Batch:  312 (1024 samples), Train Loss:      9090.86
Epoch: 68/12000, Batch:  390 (1024 samples), Train Loss:      8165.98
Epoch: 68/12000, Time elapsed/remaining/total: 4.18/733.89/738.07 min, Train Loss:      9193.01, Val MRE: 3.20%
Epoch: 69/12000, Batch:   78 (1024 samples), Train Loss:      8312.02
Epoch: 69/12000, Batch:  156 (1024 samples), Train Loss:      8163.14
Epoch: 69/12000, Batch:  234 (1024 samples), Train Loss:     10183.08
Epoch: 69/12000, Batch:  312 (1024 samples), Train Loss:      9693.02
Epoch: 69/12000, Batch:  390 (1024 samples), Train Loss:     10151.42
Epoch: 69/12000, Time elapsed/remaining/total: 4.24/733.83/738.07 min, Train Loss:      9066.82, Val MRE: 3.12%
Epoch: 70/12000, Batch:   78 (1024 samples), Train Loss:      8616.69
Epoch: 70/12000, Batch:  156 (1024 samples), Train Loss:      7784.72
Epoch: 70/12000, Batch:  234 (1024 samples), Train Loss:      8835.38
Epoch: 70/12000, Batch:  312 (1024 samples), Train Loss:      9593.37
Epoch: 70/12000, Batch:  390 (1024 samples), Train Loss:      9745.75
Epoch: 70/12000, Time elapsed/remaining/total: 4.31/733.86/738.16 min, Train Loss:      9035.39, Val MRE: 3.01%
Epoch: 71/12000, Batch:   78 (1024 samples), Train Loss:      8203.65
Epoch: 71/12000, Batch:  156 (1024 samples), Train Loss:      8781.13
Epoch: 71/12000, Batch:  234 (1024 samples), Train Loss:      8110.41
Epoch: 71/12000, Batch:  312 (1024 samples), Train Loss:      7894.37
Epoch: 71/12000, Batch:  390 (1024 samples), Train Loss:      8321.42
Epoch: 71/12000, Time elapsed/remaining/total: 4.37/734.00/738.37 min, Train Loss:      8825.45, Val MRE: 3.05%
Epoch: 72/12000, Batch:   78 (1024 samples), Train Loss:      9245.04
Epoch: 72/12000, Batch:  156 (1024 samples), Train Loss:      9054.07
Epoch: 72/12000, Batch:  234 (1024 samples), Train Loss:      9586.94
Epoch: 72/12000, Batch:  312 (1024 samples), Train Loss:     10289.17
Epoch: 72/12000, Batch:  390 (1024 samples), Train Loss:      9209.66
Epoch: 72/12000, Time elapsed/remaining/total: 4.43/734.09/738.52 min, Train Loss:      8620.16, Val MRE: 3.08%
Epoch: 73/12000, Batch:   78 (1024 samples), Train Loss:      9515.20
Epoch: 73/12000, Batch:  156 (1024 samples), Train Loss:      8538.03
Epoch: 73/12000, Batch:  234 (1024 samples), Train Loss:      7583.60
Epoch: 73/12000, Batch:  312 (1024 samples), Train Loss:      8714.55
Epoch: 73/12000, Batch:  390 (1024 samples), Train Loss:      7847.80
Epoch: 73/12000, Time elapsed/remaining/total: 4.49/733.95/738.44 min, Train Loss:      8360.39, Val MRE: 2.96%
Epoch: 74/12000, Batch:   78 (1024 samples), Train Loss:      7911.96
Epoch: 74/12000, Batch:  156 (1024 samples), Train Loss:      7891.92
Epoch: 74/12000, Batch:  234 (1024 samples), Train Loss:      8438.93
Epoch: 74/12000, Batch:  312 (1024 samples), Train Loss:      8020.59
Epoch: 74/12000, Batch:  390 (1024 samples), Train Loss:      8898.19
Epoch: 74/12000, Time elapsed/remaining/total: 4.55/733.81/738.37 min, Train Loss:      8182.03, Val MRE: 3.00%
Epoch: 75/12000, Batch:   78 (1024 samples), Train Loss:      7898.13
Epoch: 75/12000, Batch:  156 (1024 samples), Train Loss:      8397.70
Epoch: 75/12000, Batch:  234 (1024 samples), Train Loss:      8816.47
Epoch: 75/12000, Batch:  312 (1024 samples), Train Loss:      8384.15
Epoch: 75/12000, Batch:  390 (1024 samples), Train Loss:      7661.18
Epoch: 75/12000, Time elapsed/remaining/total: 4.61/733.67/738.28 min, Train Loss:      8326.25, Val MRE: 2.95%
Epoch: 76/12000, Batch:   78 (1024 samples), Train Loss:      9057.02
Epoch: 76/12000, Batch:  156 (1024 samples), Train Loss:      9781.11
Epoch: 76/12000, Batch:  234 (1024 samples), Train Loss:      7783.57
Epoch: 76/12000, Batch:  312 (1024 samples), Train Loss:      7897.19
Epoch: 76/12000, Batch:  390 (1024 samples), Train Loss:      7572.71
Epoch: 76/12000, Time elapsed/remaining/total: 4.67/733.42/738.10 min, Train Loss:      8229.33, Val MRE: 2.95%
Epoch: 77/12000, Batch:   78 (1024 samples), Train Loss:      7682.30
Epoch: 77/12000, Batch:  156 (1024 samples), Train Loss:      7899.60
Epoch: 77/12000, Batch:  234 (1024 samples), Train Loss:      7724.14
Epoch: 77/12000, Batch:  312 (1024 samples), Train Loss:      8450.53
Epoch: 77/12000, Batch:  390 (1024 samples), Train Loss:      8027.70
Epoch: 77/12000, Time elapsed/remaining/total: 4.74/733.51/738.25 min, Train Loss:      8074.65, Val MRE: 2.86%
Epoch: 78/12000, Batch:   78 (1024 samples), Train Loss:      7751.13
Epoch: 78/12000, Batch:  156 (1024 samples), Train Loss:      7904.90
Epoch: 78/12000, Batch:  234 (1024 samples), Train Loss:      8171.20
Epoch: 78/12000, Batch:  312 (1024 samples), Train Loss:      8551.12
Epoch: 78/12000, Batch:  390 (1024 samples), Train Loss:      7428.37
Epoch: 78/12000, Time elapsed/remaining/total: 4.80/733.41/738.21 min, Train Loss:      7876.65, Val MRE: 2.86%
Epoch: 79/12000, Batch:   78 (1024 samples), Train Loss:      7654.41
Epoch: 79/12000, Batch:  156 (1024 samples), Train Loss:      8434.81
Epoch: 79/12000, Batch:  234 (1024 samples), Train Loss:      7638.81
Epoch: 79/12000, Batch:  312 (1024 samples), Train Loss:      7748.04
Epoch: 79/12000, Batch:  390 (1024 samples), Train Loss:      8347.80
Epoch: 79/12000, Time elapsed/remaining/total: 4.86/733.34/738.20 min, Train Loss:      7856.25, Val MRE: 2.91%
Epoch: 80/12000, Batch:   78 (1024 samples), Train Loss:      7817.82
Epoch: 80/12000, Batch:  156 (1024 samples), Train Loss:      8018.11
Epoch: 80/12000, Batch:  234 (1024 samples), Train Loss:      7126.51
Epoch: 80/12000, Batch:  312 (1024 samples), Train Loss:      7474.60
Epoch: 80/12000, Batch:  390 (1024 samples), Train Loss:      8176.11
Epoch: 80/12000, Time elapsed/remaining/total: 4.92/733.26/738.18 min, Train Loss:      7586.01, Val MRE: 2.87%
Epoch: 81/12000, Batch:   78 (1024 samples), Train Loss:      7762.10
Epoch: 81/12000, Batch:  156 (1024 samples), Train Loss:      7678.04
Epoch: 81/12000, Batch:  234 (1024 samples), Train Loss:      7013.42
Epoch: 81/12000, Batch:  312 (1024 samples), Train Loss:      7929.89
Epoch: 81/12000, Batch:  390 (1024 samples), Train Loss:      6969.72
Epoch: 81/12000, Time elapsed/remaining/total: 4.98/732.88/737.86 min, Train Loss:      7627.86, Val MRE: 2.87%
Epoch: 82/12000, Batch:   78 (1024 samples), Train Loss:      7455.63
Epoch: 82/12000, Batch:  156 (1024 samples), Train Loss:      6735.68
Epoch: 82/12000, Batch:  234 (1024 samples), Train Loss:      8122.69
Epoch: 82/12000, Batch:  312 (1024 samples), Train Loss:      6828.24
Epoch: 82/12000, Batch:  390 (1024 samples), Train Loss:      7853.46
Epoch: 82/12000, Time elapsed/remaining/total: 5.04/732.50/737.54 min, Train Loss:      7583.00, Val MRE: 2.90%
Time limit of 5.0 minutes reached. Stopping training.
Optimization Finished!
Precomputation time: 5.04 minutes
[92mSaving model: ../results/v39-real_workload_perturb_500k/saved_models/ANEDA_W_Jinan_real_workload_perturb_500k.pt[0m
  - Model size: 2.18 MB
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/loss_history_ANEDA_W_Jinan_real_workload_perturb_500k.png[0m
Evaluation on train Finished!
Evaluation time: 0.02 minutes
Mean Absolute Error: 63.44
Mean Relative Error: 2.17%
Query time per sample: 0.659 microseconds
Adjusted query time per sample: 2.983 microseconds
Bucket 1: 1 - 5830, Local MRE: 2.68%, Fraction of data samples in this bucket: 67.59%
Bucket 2: 5830 - 11660, Local MRE: 1.11%, Fraction of data samples in this bucket: 30.46%
Bucket 3: 11660 - 17489, Local MRE: 0.94%, Fraction of data samples in this bucket: 1.87%
Bucket 4: 17489 - 23319, Local MRE: 0.83%, Fraction of data samples in this bucket: 0.08%
Bucket 5: 23319 - 29148, Local MRE: 0.91%, Fraction of data samples in this bucket: 0.01%
Total Buckets: 5, Global MRE: 2.17%, Global Count: 400000
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/mre_boxplot_ANEDA_W_Jinan_real_workload_perturb_500k_Train.png[0m
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/predictions_targets_ANEDA_W_Jinan_real_workload_perturb_500k_Train.png[0m
Evaluation on test Finished!
Evaluation time: 0.13 minutes
Mean Absolute Error: 78.02
Mean Relative Error: 2.90%
Query time per sample: 0.012 microseconds
Adjusted query time per sample: 7.757 microseconds
Bucket 1: 1 - 5397, Local MRE: 3.83%, Fraction of data samples in this bucket: 62.35%
Bucket 2: 5397 - 10793, Local MRE: 1.34%, Fraction of data samples in this bucket: 34.48%
Bucket 3: 10793 - 16190, Local MRE: 1.40%, Fraction of data samples in this bucket: 3.02%
Bucket 4: 16190 - 21586, Local MRE: 2.95%, Fraction of data samples in this bucket: 0.13%
Bucket 5: 21586 - 26982, Local MRE: 6.09%, Fraction of data samples in this bucket: 0.01%
Total Buckets: 5, Global MRE: 2.90%, Global Count: 1000000
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/mre_boxplot_ANEDA_W_Jinan_real_workload_perturb_500k_Test.png[0m
[92mSaving plot: ../results/v39-real_workload_perturb_500k/plots/predictions_targets_ANEDA_W_Jinan_real_workload_perturb_500k_Test.png[0m
