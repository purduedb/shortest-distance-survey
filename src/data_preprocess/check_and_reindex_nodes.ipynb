{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import config\n",
    "from utils.data_utils import (\n",
    "    seed_everything,\n",
    "    get_num_workers,\n",
    "    get_edge_attributes,\n",
    "    get_node_attributes,\n",
    "    load_graph,\n",
    ")\n",
    "from utils.plot_utils import (\n",
    "    plot_data_distribution,\n",
    "    plot_learning_curves,\n",
    "    plot_targets_and_predictions,\n",
    "    plot_targets_and_mre_boxplots,\n",
    ")\n",
    "from utils.torch_utils import (\n",
    "    get_available_device,\n",
    "    get_optimizer,\n",
    "    get_criterion,\n",
    "    save_dataset,\n",
    "    load_dataset,\n",
    "    save_model,\n",
    "    save_dictionary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dde78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: W_Beijing\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 74383\n",
      "  - Number of edges: 206894\n",
      "  - Number of queryies: 16505 (equivalent to 0.2 landmarks)\n",
      "  - Groundtruth contains 8.47% of nodes\n",
      "Processing dataset: W_Chengdu\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 17567\n",
      "  - Number of edges: 50590\n",
      "  - Number of queryies: 19033 (equivalent to 1.1 landmarks)\n",
      "  - Groundtruth contains 24.08% of nodes\n",
      "Processing dataset: W_Chicago\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 386533\n",
      "  - Number of edges: 1099270\n",
      "  - Number of queryies: 774289 (equivalent to 2.0 landmarks)\n",
      "  - Groundtruth contains 0.07% of nodes\n",
      "Processing dataset: W_Jinan\n",
      "  - Nodes are indexed from 0 to n-1\n",
      "  - Edges are indexed from 0 to n-1\n",
      "  - Number of nodes: 8908\n",
      "  - Number of edges: 28140\n",
      "  - Number of queryies: 117726 (equivalent to 13.2 landmarks)\n",
      "  - Groundtruth contains 12.81% of nodes\n",
      "Processing dataset: W_NewYork\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 334930\n",
      "  - Number of edges: 891866\n",
      "  - Number of queryies: 2739612 (equivalent to 8.2 landmarks)\n",
      "  - Groundtruth contains 4.09% of nodes\n",
      "Processing dataset: W_Shenzhen\n",
      "  - Nodes are indexed from 0 to n-1\n",
      "  - Edges are indexed from 0 to n-1\n",
      "  - Number of nodes: 11933\n",
      "  - Number of edges: 37808\n",
      "  - Number of queryies: 90273 (equivalent to 7.6 landmarks)\n",
      "  - Groundtruth contains 5.50% of nodes\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "data_names = [\"W_Beijing\", \"W_Chengdu\", \"W_Chicago\", \"W_Jinan\", \"W_NewYork\", \"W_Shenzhen\"]\n",
    "\n",
    "for data_name in data_names[:]:\n",
    "    print(f\"Processing dataset: {data_name}\")\n",
    "    node_path = f\"{data_dir}/{data_name}/{data_name}.nodes\"\n",
    "    edge_path = f\"{data_dir}/{data_name}/{data_name}.edges\"\n",
    "    query_path = f\"{data_dir}/{data_name}/{data_name}.queries\"\n",
    "    groundtruth_path = f\"{data_dir}/{data_name}/{data_name}.groundtruth\"\n",
    "\n",
    "    for path in [node_path, edge_path, query_path, groundtruth_path]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File does not exist: {path}\")\n",
    "\n",
    "    # Read node_path and get num nodes and check if all nodes are from 0 to n-1\n",
    "    nodes = pd.read_csv(node_path, sep=\",\", header=None)\n",
    "    node_list_from_nodes = nodes[0].sort_values().tolist()\n",
    "    num_nodes_from_nodes = len(node_list_from_nodes)\n",
    "    assert len(nodes) == num_nodes_from_nodes, \"Error\"\n",
    "    if (node_list_from_nodes == list(range(num_nodes_from_nodes))):\n",
    "        print(\"  - Nodes are indexed from 0 to n-1\")\n",
    "    elif (node_list_from_nodes == list(range(1, num_nodes_from_nodes + 1))):\n",
    "        print(\"  - Nodes are indexed from 1 to n\")\n",
    "    else:\n",
    "        print(\"  - Node IDs are not continuous\")\n",
    "\n",
    "    # Read edge_path and get num nodes and check node range as done previously\n",
    "    edges = pd.read_csv(edge_path, sep=\",\", header=None, comment=\"#\")\n",
    "    node_list_from_edges = np.sort(np.unique(edges[[0, 1]].values.ravel())).tolist()\n",
    "    num_nodes_from_edges = len(node_list_from_edges)\n",
    "    if node_list_from_edges == list(range(num_nodes_from_edges)):\n",
    "        print(\"  - Edges are indexed from 0 to n-1\")\n",
    "    elif node_list_from_edges == list(range(1, num_nodes_from_edges + 1)):\n",
    "        print(\"  - Edges are indexed from 1 to n\")\n",
    "    else:\n",
    "        print(\"  - Edge IDs are not continuous\")\n",
    "\n",
    "    assert node_list_from_nodes == node_list_from_edges, \"Node IDs do not match between nodes and edges\"\n",
    "    assert num_nodes_from_nodes == num_nodes_from_edges, \"Number of nodes do not match between nodes and edges\"\n",
    "\n",
    "    # Read groundtruth_path and get compare same stats\n",
    "    groundtruth = pd.read_csv(groundtruth_path, sep=\",\", header=None)\n",
    "    node_list_from_groundtruth = np.sort(np.unique(groundtruth[[0, 1]].values.ravel())).tolist()\n",
    "    num_nodes_from_groundtruth = len(node_list_from_groundtruth)\n",
    "    assert set(node_list_from_groundtruth).issubset(set(node_list_from_nodes)), \"Groundtruth node IDs are not a subset of node IDs\"\n",
    "    print(f\"  - Number of nodes: {len(nodes)}\")\n",
    "    print(f\"  - Number of edges: {len(edges)}\")\n",
    "    print(f\"  - Number of queryies: {len(groundtruth)} (equivalent to {len(groundtruth)/num_nodes_from_nodes:.1f} landmarks)\")\n",
    "    print(f\"  - Groundtruth contains {num_nodes_from_groundtruth/num_nodes_from_nodes*100:.2f}% of nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_nodes_edges_groundtruth(nodes_df, edges_df, groundtruth_df):\n",
    "    # Create mapping from old node IDs to new node IDs (0 to n-1)\n",
    "    old_ids = np.sort(np.unique(nodes_df[0].values)).tolist()  # generate node IDs from nodes\n",
    "    # old_ids = np.sort(np.unique(edges[[0, 1]].values.ravel())).tolist()  # generate edge IDs from edges\n",
    "    id_map = {old_id: new_id+1 for new_id, old_id in enumerate(old_ids)}\n",
    "\n",
    "    # Reindex nodes\n",
    "    nodes_df_reindexed = nodes_df.copy()\n",
    "    nodes_df_reindexed[0] = nodes_df_reindexed[0].map(id_map)\n",
    "\n",
    "    # Reindex edges\n",
    "    edges_df_reindexed = edges_df.copy()\n",
    "    edges_df_reindexed[0] = edges_df_reindexed[0].map(id_map)\n",
    "    edges_df_reindexed[1] = edges_df_reindexed[1].map(id_map)\n",
    "\n",
    "    # Reindex groundtruth\n",
    "    groundtruth_df_reindexed = groundtruth_df.copy()\n",
    "    groundtruth_df_reindexed[0] = groundtruth_df_reindexed[0].map(id_map)\n",
    "    groundtruth_df_reindexed[1] = groundtruth_df_reindexed[1].map(id_map)\n",
    "\n",
    "    return nodes_df_reindexed, edges_df_reindexed, groundtruth_df_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbaf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing W_Shenzhen...\n",
      "Reindexed and saved: W_Shenzhen\n",
      "Processing W_Jinan...\n",
      "Reindexed and saved: W_Jinan\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "for data_name in [\"W_Shenzhen\", \"W_Jinan\"]:\n",
    "    print(f\"Processing {data_name}...\")\n",
    "    node_path = f\"{data_dir}/{data_name}/{data_name}.nodes\"\n",
    "    edge_path = f\"{data_dir}/{data_name}/{data_name}.edges\"\n",
    "    groundtruth_path = f\"{data_dir}/{data_name}/{data_name}.groundtruth\"\n",
    "\n",
    "    # Load data\n",
    "    nodes_df = pd.read_csv(node_path, sep=\",\", header=None)\n",
    "    edges_df = pd.read_csv(edge_path, sep=\",\", header=None)\n",
    "    groundtruth_df = pd.read_csv(groundtruth_path, sep=\",\", header=None)\n",
    "\n",
    "    # Reindex\n",
    "    nodes_new, edges_new, groundtruth_new = reindex_nodes_edges_groundtruth(nodes_df, edges_df, groundtruth_df)\n",
    "\n",
    "    # Save back (overwrite original files)\n",
    "    nodes_new.to_csv(node_path, index=False, header=False)\n",
    "    edges_new.to_csv(edge_path, index=False, header=False)\n",
    "    groundtruth_new.to_csv(groundtruth_path, index=False, header=False)\n",
    "    print(f\"Reindexed and saved: {data_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c991c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: W_Jinan\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 8908\n",
      "  - Number of edges: 28140\n",
      "  - Number of queryies: 117726 (Num nodes: 1141) (equivalent to 13.2 landmarks)\n",
      "  - Groundtruth contains 12.81% of nodes\n",
      "Processing dataset: W_Shenzhen\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 11933\n",
      "  - Number of edges: 37808\n",
      "  - Number of queryies: 90273 (Num nodes: 656) (equivalent to 7.6 landmarks)\n",
      "  - Groundtruth contains 5.50% of nodes\n",
      "Processing dataset: W_Chengdu\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 17567\n",
      "  - Number of edges: 50590\n",
      "  - Number of queryies: 19033 (Num nodes: 4231) (equivalent to 1.1 landmarks)\n",
      "  - Groundtruth contains 24.08% of nodes\n",
      "Processing dataset: W_Beijing\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 74383\n",
      "  - Number of edges: 206894\n",
      "  - Number of queryies: 16505 (Num nodes: 6300) (equivalent to 0.2 landmarks)\n",
      "  - Groundtruth contains 8.47% of nodes\n",
      "Processing dataset: W_NewYork\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 334930\n",
      "  - Number of edges: 891866\n",
      "  - Number of queryies: 2739612 (Num nodes: 13688) (equivalent to 8.2 landmarks)\n",
      "  - Groundtruth contains 4.09% of nodes\n",
      "Processing dataset: W_Chicago\n",
      "  - Nodes are indexed from 1 to n\n",
      "  - Edges are indexed from 1 to n\n",
      "  - Number of nodes: 386533\n",
      "  - Number of edges: 1099270\n",
      "  - Number of queryies: 774289 (Num nodes: 285) (equivalent to 2.0 landmarks)\n",
      "  - Groundtruth contains 0.07% of nodes\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "data_names = [\n",
    "    \"W_Jinan\",\n",
    "    \"W_Shenzhen\",\n",
    "    \"W_Chengdu\",\n",
    "    \"W_Beijing\",\n",
    "    \"W_NewYork\",\n",
    "    \"W_Chicago\",\n",
    "]\n",
    "\n",
    "for data_name in data_names[:]:\n",
    "    print(f\"Processing dataset: {data_name}\")\n",
    "    node_path = f\"{data_dir}/{data_name}/{data_name}.nodes\"\n",
    "    edge_path = f\"{data_dir}/{data_name}/{data_name}.edges\"\n",
    "    query_path = f\"{data_dir}/{data_name}/{data_name}.queries\"\n",
    "    groundtruth_path = f\"{data_dir}/{data_name}/{data_name}.groundtruth\"\n",
    "\n",
    "    for path in [node_path, edge_path, query_path, groundtruth_path]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File does not exist: {path}\")\n",
    "\n",
    "    # Read node_path and get num nodes and check if all nodes are from 0 to n-1\n",
    "    nodes = pd.read_csv(node_path, sep=\",\", header=None)\n",
    "    node_list_from_nodes = nodes[0].sort_values().tolist()\n",
    "    num_nodes_from_nodes = len(node_list_from_nodes)\n",
    "    assert len(nodes) == num_nodes_from_nodes, \"Error\"\n",
    "    if (node_list_from_nodes == list(range(num_nodes_from_nodes))):\n",
    "        print(\"  - Nodes are indexed from 0 to n-1\")\n",
    "    elif (node_list_from_nodes == list(range(1, num_nodes_from_nodes + 1))):\n",
    "        print(\"  - Nodes are indexed from 1 to n\")\n",
    "    else:\n",
    "        print(\"  - Node IDs are not continuous\")\n",
    "\n",
    "    # Read edge_path and get num nodes and check node range as done previously\n",
    "    edges = pd.read_csv(edge_path, sep=\",\", header=None, comment=\"#\")\n",
    "    node_list_from_edges = np.sort(np.unique(edges[[0, 1]].values.ravel())).tolist()\n",
    "    num_nodes_from_edges = len(node_list_from_edges)\n",
    "    if node_list_from_edges == list(range(num_nodes_from_edges)):\n",
    "        print(\"  - Edges are indexed from 0 to n-1\")\n",
    "    elif node_list_from_edges == list(range(1, num_nodes_from_edges + 1)):\n",
    "        print(\"  - Edges are indexed from 1 to n\")\n",
    "    else:\n",
    "        print(\"  - Edge IDs are not continuous\")\n",
    "\n",
    "    assert node_list_from_nodes == node_list_from_edges, \"Node IDs do not match between nodes and edges\"\n",
    "    assert num_nodes_from_nodes == num_nodes_from_edges, \"Number of nodes do not match between nodes and edges\"\n",
    "\n",
    "    # Read groundtruth_path and get compare same stats\n",
    "    groundtruth = pd.read_csv(groundtruth_path, sep=\",\", header=None)\n",
    "    node_list_from_groundtruth = np.sort(np.unique(groundtruth[[0, 1]].values.ravel())).tolist()\n",
    "    num_nodes_from_groundtruth = len(node_list_from_groundtruth)\n",
    "    assert set(node_list_from_groundtruth).issubset(set(node_list_from_nodes)), \"Groundtruth node IDs are not a subset of node IDs\"\n",
    "    print(f\"  - Number of nodes: {len(nodes)}\")\n",
    "    print(f\"  - Number of edges: {len(edges)}\")\n",
    "    print(f\"  - Number of queryies: {len(groundtruth)} (Num nodes: {len(node_list_from_groundtruth)}) (equivalent to {len(groundtruth)/num_nodes_from_nodes:.1f} landmarks)\")\n",
    "    print(f\"  - Groundtruth contains {num_nodes_from_groundtruth/num_nodes_from_nodes*100:.2f}% of nodes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
