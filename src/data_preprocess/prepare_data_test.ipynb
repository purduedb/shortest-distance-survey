{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34a9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "  - data_name: Moscow\n",
      "  - data_strategy: landmark\n",
      "  - data_version: v0\n",
      "  - landmarks: 100\n",
      "  - random_pairs: 1000000\n",
      "  - seed: 42\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import argparse\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "from data_utils import (\n",
    "    seed_everything,\n",
    "    read_figshare_data,\n",
    "    graph_from_edgelist,\n",
    "    preprocess_graph,\n",
    "    get_edge_attributes,\n",
    "    get_node_attributes,\n",
    "    save_graph,\n",
    "    AllPairsDataset,\n",
    "    LandmarkPairsDataset,\n",
    "    RandomPairsDataset,\n",
    "    save_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Parse command line arguments\n",
    "parser = argparse.ArgumentParser(description='Prepare data for shortest distance calculation.')\n",
    "parser.add_argument('--data_name', type=str, default='Surat',\n",
    "                    help='Name of the dataset')\n",
    "parser.add_argument('--data_strategy', type=str, default='landmark',\n",
    "                    choices=['all', 'landmark', 'random'],\n",
    "                    help='Strategy for creating dataset')\n",
    "parser.add_argument('--data_version', type=str, default='v0',\n",
    "                    help='Version of the preprocessed dataset (e.g., v1, v2, etc.)')\n",
    "parser.add_argument('--landmarks', type=float, default=100,\n",
    "                    help='Number of landmarks for landmark strategy. If >=1, treated as an absolute \\\n",
    "                    number. If <1, treated as a percentage of total nodes.')\n",
    "parser.add_argument('--random_pairs', type=int, default=1_000_000,\n",
    "                    help='Number of random pairs for random strategy')\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help='Random seed for reproducibility')\n",
    "args = parser.parse_args([\n",
    "    '--data_name', 'Moscow',\n",
    "])\n",
    "print(\"Arguments:\")\n",
    "for arg, value in vars(args).items():\n",
    "    print(f\"  - {arg}: {value}\")\n",
    "\n",
    "# Get arguments\n",
    "data_name = args.data_name\n",
    "data_strategy = args.data_strategy\n",
    "data_version = args.data_version\n",
    "landmarks = args.landmarks\n",
    "random_pairs = args.random_pairs\n",
    "seed = args.seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8635d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mReading data: ../data/raw/figshare-2061897/Moscow.zip\u001b[0m\n",
      "Finished reading data: Moscow\n",
      "\u001b[93mWarning: Converting edge lengths to integers.\u001b[0m\n",
      "Before conversion:\n",
      "          XCoord        YCoord  START_NODE  END_NODE  EDGE      LENGTH\n",
      "0  480133.684133  6.009245e+06           1         2     1  386.540729\n",
      "1  480330.093606  6.009578e+06           2         1     1  386.540729\n",
      "2  480330.093606  6.009578e+06           2         4     2  263.981363\n",
      "3  480330.093606  6.009578e+06           2         6     3   98.542220\n",
      "4  480549.811762  6.009453e+06           4         2     2  263.981363\n",
      "After conversion:\n",
      "          XCoord        YCoord  START_NODE  END_NODE  EDGE  LENGTH\n",
      "0  480133.684133  6.009245e+06           1         2     1     386\n",
      "1  480330.093606  6.009578e+06           2         1     1     386\n",
      "2  480330.093606  6.009578e+06           2         4     2     263\n",
      "3  480330.093606  6.009578e+06           2         6     3      98\n",
      "4  480549.811762  6.009453e+06           4         2     2     263\n",
      "Finished checking: graph has no self loops\n",
      "          XCoord        YCoord  START_NODE  END_NODE  EDGE  LENGTH\n",
      "0  480133.684133  6.009245e+06           1         2     1     386\n",
      "1  480330.093606  6.009578e+06           2         1     1     386\n",
      "2  480330.093606  6.009578e+06           2         4     2     263\n",
      "3  480330.093606  6.009578e+06           2         6     3      98\n",
      "4  480549.811762  6.009453e+06           4         2     2     263\n",
      "Node data in graph:  [(1, {'XCoord': 480133.68413296, 'YCoord': 6009244.7034824}), (2, {'XCoord': 480330.09360591, 'YCoord': 6009577.62519383})]\n",
      "Edge data in graph:  [(1, 2, {'LENGTH': 386}), (2, 4, {'LENGTH': 263})]\n",
      "Original graph:\n",
      "  - Node count: 692891\n",
      "  - Edge count: 930900\n",
      "  - Min/Max/Avg degree: 1/11/2.69\n",
      "  - Min/Max/Avg weight: 0.00/11963.00/128.53\n",
      "  - No. of connected components: 2204\n",
      "Taking the largest connected component...\n",
      "  - Node count: 685091\n",
      "  - Edge count: 924683\n",
      "  - Min/Max/Avg degree: 1/11/2.70\n",
      "  - Min/Max/Avg weight: 0.00/11963.00/127.96\n",
      "  - No. of connected components: 1\n",
      "\u001b[93mWarning: One-degree nodes are still present in the graph.\u001b[0m\n",
      "Re-indexing the nodes...\n",
      "Min/Max node labels before re-indexing: 1/1880454\n",
      "Min/Max node labels after re-indexing: 0/685090\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "RAW_DATA_DIR = config.FIGSHARE_DATA_DIR\n",
    "PROCESSED_DATA_DIR = os.path.join(config.PROCESSED_DATA_DIR, data_version, data_name)\n",
    "\n",
    "# Read the edgelist data for the specified dataset\n",
    "edgelist = read_figshare_data(data_name, dir_name=RAW_DATA_DIR)\n",
    "\n",
    "# Display the first few rows of the edgelist\n",
    "print(edgelist.head())\n",
    "\n",
    "# Create a graph from the edgelist\n",
    "G = graph_from_edgelist(edgelist)\n",
    "\n",
    "# Add metadata to the graph\n",
    "G.graph['data_name'] = data_name\n",
    "\n",
    "# Print the first 5 nodes with their attributes\n",
    "print(\"Node data in graph: \", list(G.nodes(data=True))[:2])\n",
    "\n",
    "# Print the first 5 edges with their attributes\n",
    "print(\"Edge data in graph: \", list(G.edges(data=True))[:2])\n",
    "\n",
    "# # Plot the graph around a specific node\n",
    "# plot_subgraph(G, 1)\n",
    "\n",
    "# # Plot the graph around max degree node with radius 2\n",
    "# max_degree_node = max(dict(G.degree()).items(), key=lambda x: x[1])[0]\n",
    "# print(f\"Max degree node: {max_degree_node} (degree={G.degree(max_degree_node)})\")\n",
    "# plot_subgraph(G, max_degree_node, radius=2)\n",
    "\n",
    "# # Plot the degree distribution before preprocessing\n",
    "# plot_degree_distribution(G)\n",
    "\n",
    "# Preprocess the graph\n",
    "G_prime = preprocess_graph(G, reindex=True)\n",
    "\n",
    "# # Plot the degree distribution after preprocessing\n",
    "# plot_degree_distribution(G_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d5a486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSaving graph: ../data/processed/v0/Moscow/Moscow_nx_graph.pkl\u001b[0m\n",
      "\u001b[93mWarning: The above graph is saved in (0 to n-1) indexing (u, v) for compatibility with PyTorch Geometric.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save the graph\n",
    "save_graph(G_prime, data_name, dir_name=PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bfbf161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 gchoudha student 12M Jul 16 00:25 ../data/processed/v0/Rome/Rome_nx_graph.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ../data/processed/v0/Rome/Rome_nx_graph.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b2af04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph written to ../data/processed/v0/Moscow/Moscow.edgelist\n",
      "Size of edgelist: 13.84 MB\n"
     ]
    }
   ],
   "source": [
    "# Write G_prime to a .edgelist file as an edgelist (src dst weight)\n",
    "file_name = os.path.join(PROCESSED_DATA_DIR, f\"{data_name}.edgelist\")\n",
    "with open(file_name, \"w\") as f:\n",
    "    for u, v, data in G_prime.edges(data=True):\n",
    "        weight = data.get(\"weight\", 1)\n",
    "        f.write(f\"{u} {v} {weight}\\n\")\n",
    "print(f\"Graph written to {file_name}\")\n",
    "\n",
    "print(f\"Size of edgelist: {os.path.getsize(file_name)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cda98d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed edgelist written to ../data/processed/v0/Moscow/Moscow.edgelist.gz\n",
      "Size of compressed edgelist: 3.37 MB\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "compressed_file_name = os.path.join(PROCESSED_DATA_DIR, f\"{data_name}.edgelist.gz\")\n",
    "with gzip.open(compressed_file_name, \"wt\") as f:\n",
    "    for u, v, data in G_prime.edges(data=True):\n",
    "        weight = data.get(\"weight\", 1)\n",
    "        f.write(f\"{u} {v} {weight}\\n\")\n",
    "print(f\"Compressed edgelist written to {compressed_file_name}\")\n",
    "\n",
    "print(f\"Size of compressed edgelist: {os.path.getsize(compressed_file_name)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "469f6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node attributes written to ../data/processed/v0/Moscow/Moscow_node_attributes.csv\n",
      "Size of node attributes file: 17.33 MB\n"
     ]
    }
   ],
   "source": [
    "# Save node attributes of G_prime to a CSV file\n",
    "node_attributes_prime = get_node_attributes(G_prime)\n",
    "node_attr_file = os.path.join(PROCESSED_DATA_DIR, f\"{data_name}_node_attributes.csv\")\n",
    "node_attributes_prime.to_csv(node_attr_file, index=True)\n",
    "print(f\"Node attributes written to {node_attr_file}\")\n",
    "\n",
    "print(f\"Size of node attributes file: {os.path.getsize(node_attr_file)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed node attributes written to ../data/processed/v0/Moscow/Moscow_node_attributes.csv.gz\n",
      "Size of compressed node attributes file: 5.77 MB\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "compressed_node_attr_file = os.path.join(PROCESSED_DATA_DIR, f\"{data_name}_node_attributes.csv.gz\")\n",
    "with gzip.open(compressed_node_attr_file, \"wt\") as f:\n",
    "    node_attributes_prime.to_csv(f, index=True)\n",
    "print(f\"Compressed node attributes written to {compressed_node_attr_file}\")\n",
    "\n",
    "print(f\"Size of compressed node attributes file: {os.path.getsize(compressed_node_attr_file)/1024/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09f8bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 745M\n",
      "drwxr-xr-x  2 gchoudha student 4.0K Jun  5 10:02 .\n",
      "drwxr-xr-x 32 gchoudha student 4.0K Jun  5 09:56 ..\n",
      "-rw-r--r--  1 gchoudha student 346M Jun  5 10:02 Moscow_landmark_test_torch_dataset.pth\n",
      "-rw-r--r--  1 gchoudha student 348M Jun  5 10:02 Moscow_landmark_train_torch_dataset.pth\n",
      "-rw-r--r--  1 gchoudha student  53M Jun  5 09:56 Moscow_nx_graph.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ../data/processed/v2-urban-landmark/Moscow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute the edge list for the graph\n",
    "edge_attributes_prime = get_edge_attributes(G_prime)\n",
    "print(\"Edgelist.shape: \", edge_attributes_prime.shape)\n",
    "print(edge_attributes_prime.head())\n",
    "\n",
    "# Compute the node attributes for the graph\n",
    "node_attributes_prime = get_node_attributes(G_prime)\n",
    "print(\"Node Attributes.shape: \", node_attributes_prime.shape)\n",
    "print(node_attributes_prime.head())\n",
    "\n",
    "# Create dataset using graph\n",
    "if data_strategy == \"all\":\n",
    "    print(\"Creating train and test datasets using all pairs strategy...\")\n",
    "    train_dataset = AllPairsDataset(G_prime)\n",
    "    test_dataset = train_dataset  # Since we are using all pairs, train and test datasets are the same\n",
    "elif data_strategy == \"landmark\":\n",
    "    print(\"Creating train and test datasets using landmark strategy...\")\n",
    "    train_dataset = LandmarkPairsDataset(G_prime, l=landmarks, seed=seed+1234)  ## Using different seeds compared to model seeds\n",
    "    test_dataset = LandmarkPairsDataset(G_prime, l=landmarks, seed=seed+2345)\n",
    "elif data_strategy == \"random\":\n",
    "    print(\"Creating train and test datasets using random pairs strategy...\")\n",
    "    train_dataset = RandomPairsDataset(G_prime, k=random_pairs, seed=seed+3456)\n",
    "    test_dataset = RandomPairsDataset(G_prime, k=random_pairs, seed=seed+4567)\n",
    "else:\n",
    "    raise ValueError(\"Invalid data_strategy\")\n",
    "\n",
    "# TODO: Shall we limit the dataset size to 1M pairs?\n",
    "# e.g., train_dataset = Subset(train_dataset, 1_000_000)\n",
    "\n",
    "# Save dataset\n",
    "save_dataset(train_dataset, data_name, data_strategy, data_split=\"train\", dir_name=PROCESSED_DATA_DIR)\n",
    "save_dataset(test_dataset, data_name, data_strategy, data_split=\"test\", dir_name=PROCESSED_DATA_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
